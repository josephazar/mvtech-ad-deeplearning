{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVTec-AD Unified Evaluation Notebook\n",
    "\n",
    "This notebook provides a unified approach to evaluate multiple anomaly detection models (GLASS, DDAD, DiffusionAD, Dinomaly) on the MVTec-AD dataset.\n",
    "\n",
    "## Features:\n",
    "- Automatic dataset download\n",
    "- GPU-enabled training and evaluation\n",
    "- Unified metrics computation\n",
    "- Results visualization\n",
    "- Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"Running in local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository if in Colab\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/Christian-Beddawi/Deep-Learning-for-Industrial-Image-Anomaly-Detection.git\n",
    "    %cd Deep-Learning-for-Industrial-Image-Anomaly-Detection\n",
    "    !ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q scikit-learn scikit-image opencv-python-headless\n",
    "!pip install -q tqdm pandas numpy matplotlib seaborn\n",
    "!pip install -q pyyaml click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download MVTec-AD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory based on environment\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = \"/content/drive/MyDrive/mvtec_data\"\n",
    "else:\n",
    "    DATA_DIR = \"./datasets\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MVTec-AD dataset\n",
    "sys.path.append('./scripts')\n",
    "from download_mvtec import download_mvtec_ad\n",
    "\n",
    "# Download dataset (will skip if already exists)\n",
    "dataset_path = download_mvtec_ad(data_dir=DATA_DIR, force_download=False)\n",
    "print(f\"\\nDataset ready at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Evaluation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"configs/unified_config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update dataset path\n",
    "config['dataset']['path'] = str(dataset_path)\n",
    "\n",
    "# Configure which models to evaluate\n",
    "EVALUATE_MODELS = {\n",
    "    'glass': True,\n",
    "    'ddad': True,\n",
    "    'diffusion_ad': True,\n",
    "    'dinomaly': True\n",
    "}\n",
    "\n",
    "# Update config\n",
    "for model_name, enabled in EVALUATE_MODELS.items():\n",
    "    if model_name in config['models']:\n",
    "        config['models'][model_name]['enabled'] = enabled\n",
    "\n",
    "# Configure evaluation settings\n",
    "config['evaluation']['batch_size'] = 16 if torch.cuda.is_available() else 4\n",
    "config['evaluation']['num_workers'] = 2 if IN_COLAB else 4\n",
    "\n",
    "print(\"Configuration updated:\")\n",
    "print(f\"  Dataset path: {config['dataset']['path']}\")\n",
    "print(f\"  Batch size: {config['evaluation']['batch_size']}\")\n",
    "print(f\"  Models to evaluate: {[k for k, v in EVALUATE_MODELS.items() if v]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Unified Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation module\n",
    "from unified_evaluation import UnifiedEvaluator\n",
    "\n",
    "# Save temporary config\n",
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:\n",
    "    yaml.dump(config, f)\n",
    "    temp_config_path = f.name\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = UnifiedEvaluator(temp_config_path)\n",
    "\n",
    "print(\"\\nStarting evaluation...\")\n",
    "print(\"This may take a while depending on the number of models and GPU availability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for all models\n",
    "start_time = time.time()\n",
    "results = evaluator.evaluate_all_models()\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTotal evaluation time: {total_time/60:.2f} minutes\")\n",
    "\n",
    "# Clean up temp config\n",
    "os.unlink(temp_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    if 'overall' in model_results:\n",
    "        row = {'Model': model_name.upper()}\n",
    "        # Extract key metrics\n",
    "        for metric in ['image_auroc_mean', 'pixel_auroc_mean', 'pro_score_mean']:\n",
    "            if metric in model_results['overall']:\n",
    "                row[metric.replace('_mean', '').replace('_', ' ').title()] = model_results['overall'][metric]\n",
    "        summary_data.append(row)\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary = df_summary.round(4)\n",
    "\n",
    "print(\"\\nEVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results by category\n",
    "detailed_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    for category, cat_results in model_results.items():\n",
    "        if category != 'overall' and isinstance(cat_results, dict) and 'error' not in cat_results:\n",
    "            row = {\n",
    "                'Model': model_name.upper(),\n",
    "                'Category': category,\n",
    "            }\n",
    "            # Extract metrics\n",
    "            for metric in ['image_auroc', 'pixel_auroc', 'pro_score']:\n",
    "                if metric in cat_results:\n",
    "                    row[metric.replace('_', ' ').title()] = cat_results[metric]\n",
    "            detailed_data.append(row)\n",
    "\n",
    "df_detailed = pd.DataFrame(detailed_data)\n",
    "\n",
    "# Show sample of detailed results\n",
    "print(\"\\nDETAILED RESULTS (first 10 rows)\")\n",
    "print(\"=\"*50)\n",
    "print(df_detailed.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of models across metrics\n",
    "if not df_summary.empty:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics_to_plot = ['Image Auroc', 'Pixel Auroc', 'Pro Score']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        if metric in df_summary.columns:\n",
    "            ax = axes[idx]\n",
    "            df_summary.plot(x='Model', y=metric, kind='bar', ax=ax, legend=False)\n",
    "            ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Model', fontsize=12)\n",
    "            ax.set_ylabel(metric, fontsize=12)\n",
    "            ax.set_ylim([0.8, 1.0] if 'auroc' in metric.lower() else [0.6, 1.0])\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for container in ax.containers:\n",
    "                ax.bar_label(container, fmt='%.3f')\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison on MVTec-AD', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance by category\n",
    "if not df_detailed.empty:\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Pivot data for heatmap\n",
    "    metric_to_show = 'Image Auroc'\n",
    "    if metric_to_show in df_detailed.columns:\n",
    "        pivot_data = df_detailed.pivot(index='Category', columns='Model', values=metric_to_show)\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                    vmin=0.8, vmax=1.0, cbar_kws={'label': metric_to_show})\n",
    "        plt.title(f'{metric_to_show} Performance Across Categories', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Model', fontsize=12)\n",
    "        plt.ylabel('Category', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot category-wise performance distribution\n",
    "if not df_detailed.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    models = df_detailed['Model'].unique()\n",
    "    \n",
    "    for idx, model in enumerate(models[:4]):\n",
    "        ax = axes[idx]\n",
    "        model_data = df_detailed[df_detailed['Model'] == model]\n",
    "        \n",
    "        if 'Image Auroc' in model_data.columns:\n",
    "            model_data.plot(x='Category', y='Image Auroc', kind='bar', ax=ax, legend=False)\n",
    "            ax.set_title(f'{model} Performance by Category', fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel('Category', fontsize=10)\n",
    "            ax.set_ylabel('Image AUROC', fontsize=10)\n",
    "            ax.set_ylim([0.7, 1.0])\n",
    "            ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Category-wise Performance Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistical significance (if multiple runs available)\n",
    "print(\"\\nSTATISTICAL ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compute variance and confidence intervals\n",
    "for model_name, model_results in results.items():\n",
    "    if 'overall' in model_results:\n",
    "        print(f\"\\n{model_name.upper()}:\")\n",
    "        for metric in ['image_auroc', 'pixel_auroc', 'pro_score']:\n",
    "            mean_key = f\"{metric}_mean\"\n",
    "            std_key = f\"{metric}_std\"\n",
    "            \n",
    "            if mean_key in model_results['overall'] and std_key in model_results['overall']:\n",
    "                mean_val = model_results['overall'][mean_key]\n",
    "                std_val = model_results['overall'][std_key]\n",
    "                \n",
    "                # 95% confidence interval\n",
    "                ci_lower = mean_val - 1.96 * std_val / np.sqrt(15)  # 15 categories\n",
    "                ci_upper = mean_val + 1.96 * std_val / np.sqrt(15)\n",
    "                \n",
    "                print(f\"  {metric}:\")\n",
    "                print(f\"    Mean: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "                print(f\"    95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best performing model for each category\n",
    "if not df_detailed.empty and 'Image Auroc' in df_detailed.columns:\n",
    "    print(\"\\nBEST MODEL PER CATEGORY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    categories = df_detailed['Category'].unique()\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_data = df_detailed[df_detailed['Category'] == category]\n",
    "        if not cat_data.empty:\n",
    "            best_idx = cat_data['Image Auroc'].idxmax()\n",
    "            best_model = cat_data.loc[best_idx, 'Model']\n",
    "            best_score = cat_data.loc[best_idx, 'Image Auroc']\n",
    "            print(f\"{category:15} -> {best_model:10} (AUROC: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to files\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save JSON results\n",
    "json_path = results_dir / f\"mvtec_results_{timestamp}.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=float)\n",
    "print(f\"Results saved to {json_path}\")\n",
    "\n",
    "# Save summary CSV\n",
    "if not df_summary.empty:\n",
    "    csv_path = results_dir / f\"mvtec_summary_{timestamp}.csv\"\n",
    "    df_summary.to_csv(csv_path, index=False)\n",
    "    print(f\"Summary saved to {csv_path}\")\n",
    "\n",
    "# Save detailed CSV\n",
    "if not df_detailed.empty:\n",
    "    csv_path = results_dir / f\"mvtec_detailed_{timestamp}.csv\"\n",
    "    df_detailed.to_csv(csv_path, index=False)\n",
    "    print(f\"Detailed results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate LaTeX Table for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for the paper\n",
    "if not df_summary.empty:\n",
    "    print(\"\\nLaTeX Table for Paper:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Format for LaTeX\n",
    "    latex_df = df_summary.copy()\n",
    "    \n",
    "    # Convert to LaTeX\n",
    "    latex_str = latex_df.to_latex(\n",
    "        index=False,\n",
    "        column_format='l' + 'c' * (len(latex_df.columns) - 1),\n",
    "        float_format=\"%.3f\",\n",
    "        caption=\"Comparison of SOTA IAD models on MVTec-AD dataset\",\n",
    "        label=\"tab:mvtec_results\"\n",
    "    )\n",
    "    \n",
    "    print(latex_str)\n",
    "    \n",
    "    # Save to file\n",
    "    latex_path = results_dir / f\"mvtec_table_{timestamp}.tex\"\n",
    "    with open(latex_path, 'w') as f:\n",
    "        f.write(latex_str)\n",
    "    print(f\"\\nLaTeX table saved to {latex_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "1. Downloaded the MVTec-AD dataset\n",
    "2. Configured the evaluation environment\n",
    "3. Evaluated multiple anomaly detection models\n",
    "4. Computed comprehensive metrics (Image AUROC, Pixel AUROC, PRO Score)\n",
    "5. Performed statistical analysis\n",
    "6. Generated visualizations\n",
    "7. Saved results in multiple formats\n",
    "\n",
    "The results are ready to be included in your paper revision!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}